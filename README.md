# CS-584-Machine-Learning

# COURSE OUTLINE

# What to expect from this course

Machine learning can be covered at different levels. The focus of this course is the understanding of algorithms and techniques used in machine learning. Students in the course are expected to write computer programs (Python) implementing different techniques taught in the course. The course requires mathematical background and some programming experience. This course does not intend to teach how to use a specific application software.



# Objectives

Introduce the fundamental problems of machine learning.
Provide understanding of techniques, mathematical concepts, and algorithms used in machine learning to facilitate further study in this area.
Provide understanding of the limitations of various machine learning algorithms and the way to evaluate performance of machine learning algorithms.
Provide pointers into the literature and exercise a project based on literature search and one or more research papers.
Practice software implementation of different concepts and algorithms covered in the course.


# Overview

Introduction: overview of machine learning, related areas, applications, software tools, course objectives.
Parametric regression: linear regression, polynomial regression, locally weighted regression, numerical optimization, gradient descent, kernel methods.
Generative learning: Gaussian parameter estimation, maximum likelihood estimation, MAP estimation, Bayesian estimation, bias and variance of estimators, missing and noisy features, nonparametric density estimation, Gaussian discriminant analysis, naive Bayes.
Discriminative learning: linear discrimination, logistic regression, logit and logistic functions, generalized linear models, softmax regression.
Neural networks: the perceptron algorithm, multilayer perceptrons, backpropagation, nonlinear regression, multiclass discrimination, training procedures, localized network structure, deep neural networks.
Support vector machines: functional and geometric margins, optimum margin classifier, constrained optimization, Lagrange multipliers, primal/dual problems, KKT conditions, dual of the optimum margin classifier, soft margins, kernels, quadratic programming, SMO algorithm.
Graphical and sequential models: Bayesian networks, conditional independence, Markov models, hidden Markov models, decoding states from observations, learning HMM parameters,Markov random fields, inference in graphical models, belief propagation,
Unsupervised learning: K-means clustering, expectation maximization, Gaussian mixture density estimation, mixture of naive Bayes, model selection.
Dimensionality reduction: feature selection, principal component analysis, linear discriminant analysis, factor analysis, independent component analysis, multidimensional scaling, manifold learning.
Reinforcement learning
Final project: students present selected topics and develop software implementation of related techniques based on the review of relevant literature. The work should be summarized in a concluding report which should include simulation results. A list of possible topics will be available prior to the project selection due date.


# Link 
http://www.cs.iit.edu/~agam/cs584/index.html
